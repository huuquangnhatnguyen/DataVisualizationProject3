{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main cast characters\n",
    "MAIN_CAST = ['Sheldon', 'Penny', 'Amy', 'Howard', 'Bernadette', 'Leonard', 'Raj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing punctuation, converting to lowercase, and removing stop words.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Tokenize and convert to lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Get English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Additional stop words specific to the script context\n",
    "    additional_stop_words = {'oh', 'uh', 'um', 'like', 'just', 'well', 'yeah', 'okay', 'ok', 'so', 'hey', 'oh', 'ah', \n",
    "                            'gonna', 'wanna', 'gotta', 'would', 'could', 'should', 'i', 'you', 'he', 'she', 'it', 'we', \n",
    "                            'they', 'me', 'him', 'her', 'them', 'my', 'your', 'his', 'its', 'our', 'their', 'that', \n",
    "                            'this', 'what', 'going', 'get', 'got', 'do', 'does', 'did', 'doing', 'don', 'doesn', \n",
    "                            'didn', 'is', 'am', 'are', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having', 'na', 'gon'}\n",
    "    stop_words.update(additional_stop_words)\n",
    "    \n",
    "    # Remove stop words and non-alphabetic characters\n",
    "    filtered_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) > 1]\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dialogue_data(file_path):\n",
    "    \"\"\"Load the dialogue data from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully with {len(df)} rows.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words_by_season(character_series_words, top_n=10):\n",
    "    \"\"\"Find words that are unique to each main cast character by season.\"\"\"\n",
    "    # Get all seasons with valid data (exclude NaN)\n",
    "    seasons = sorted(set([s for character in character_series_words \n",
    "                       for s in character_series_words[character].keys() \n",
    "                       if not pd.isna(s)]))\n",
    "    \n",
    "    all_results = []\n",
    "    id_counter = 1\n",
    "    \n",
    "    for season in seasons:\n",
    "        # Get all words used by each character in this season\n",
    "        season_character_words = {character: [] for character in MAIN_CAST}\n",
    "        all_season_words = []\n",
    "        \n",
    "        for character in MAIN_CAST:\n",
    "            if character in character_series_words and season in character_series_words[character]:\n",
    "                season_character_words[character] = character_series_words[character][season]\n",
    "                all_season_words.extend(season_character_words[character])\n",
    "        \n",
    "        # Calculate total word frequencies for this season\n",
    "        season_word_freq = Counter(all_season_words)\n",
    "        \n",
    "        # Calculate uniqueness scores for each character in this season\n",
    "        for character in MAIN_CAST:\n",
    "            if not season_character_words[character]:\n",
    "                continue\n",
    "                \n",
    "            char_word_freq = Counter(season_character_words[character])\n",
    "            \n",
    "            # Calculate uniqueness score for each word\n",
    "            uniqueness_scores = {}\n",
    "            for word, count in char_word_freq.items():\n",
    "                # Skip very rare words (used less than 2 times by this character in this season)\n",
    "                if count < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate uniqueness score - higher when the word is used more by this character\n",
    "                # and less by others in this season\n",
    "                uniqueness_scores[word] = (count / season_word_freq[word] * count) if season_word_freq[word] > 0 else 0\n",
    "            \n",
    "            # Get top N unique words for this character in this season\n",
    "            top_unique_words = sorted(uniqueness_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "            \n",
    "            # Add to the results - each word appears exactly once per character per season\n",
    "            for rank, (word, score) in enumerate(top_unique_words, 1):\n",
    "                all_results.append({\n",
    "                    'id': id_counter,\n",
    "                    'character': character,\n",
    "                    'season': season,\n",
    "                    'rank': rank,  # Add rank information (1-10)\n",
    "                    'word': word,\n",
    "                    'count': char_word_freq[word],\n",
    "                    'uniqueness_score': score\n",
    "                })\n",
    "                id_counter += 1\n",
    "    \n",
    "    return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dialogue_data(df):\n",
    "    \"\"\"Process dialogue data to extract words used by each main cast character in each series.\"\"\"\n",
    "    if 'person_scene' not in df.columns or 'dialogue' not in df.columns or 'series' not in df.columns:\n",
    "        print(\"Required columns not found in the dataframe.\")\n",
    "        return None\n",
    "    \n",
    "    # Filter for main cast and exclude scene descriptions\n",
    "    df_dialogue = df[(df['person_scene'].isin(MAIN_CAST))]\n",
    "    \n",
    "    # Check if we have any dialogue for the main cast\n",
    "    if len(df_dialogue) == 0:\n",
    "        print(\"No dialogue found for the specified main cast. Check character names in the dataset.\")\n",
    "        # Print all unique character names to help troubleshoot\n",
    "        print(\"Available characters in the dataset:\", df['person_scene'].unique())\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(df_dialogue)} dialogue lines for the main cast.\")\n",
    "    \n",
    "    # Initialize structures to store word counts\n",
    "    character_series_words = {character: {} for character in MAIN_CAST}\n",
    "    \n",
    "    # Process each row in the dataframe\n",
    "    for _, row in df_dialogue.iterrows():\n",
    "        character = row['person_scene']\n",
    "        series = row['series']\n",
    "        dialogue = row['dialogue']\n",
    "        \n",
    "        # Clean the dialogue text\n",
    "        words = clean_text(dialogue)\n",
    "        \n",
    "        # Update the character's word count for this series\n",
    "        if series not in character_series_words[character]:\n",
    "            character_series_words[character][series] = []\n",
    "        \n",
    "        character_series_words[character][series].extend(words)\n",
    "    \n",
    "    return character_series_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution flow - only outputs the requested CSV file.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    # Load the dataset\n",
    "    file_path = 'big_bang_scripts.csv'\n",
    "    dialogue_df = load_dialogue_data(file_path)\n",
    "    \n",
    "    if dialogue_df is not None:\n",
    "        # Process the dialogue data\n",
    "        character_series_words = process_dialogue_data(dialogue_df)\n",
    "        \n",
    "        if character_series_words:\n",
    "            # Get unique words by season for each character\n",
    "            season_uniqueness_df = get_unique_words_by_season(character_series_words)\n",
    "            \n",
    "            # Save the CSV file\n",
    "            output_path = 'output/testing.csv'\n",
    "            season_uniqueness_df.to_csv(output_path, index=False)\n",
    "            print(f\"Success! Saved file with {len(season_uniqueness_df)} entries to {output_path}\")\n",
    "            \n",
    "            # Verify that each character has at most 10 words per season\n",
    "            word_counts = season_uniqueness_df.groupby(['character', 'season']).size().reset_index(name='word_count')\n",
    "            if word_counts['word_count'].max() <= 10:\n",
    "                print(\"All character-season combinations have the expected number of words.\")\n",
    "            else:\n",
    "                print(\"WARNING: Some character-season combinations have more than 10 words!\")\n",
    "        else:\n",
    "            print(\"Failed to process dialogue data.\")\n",
    "    else:\n",
    "        print(\"Failed to load dialogue data.\")\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
