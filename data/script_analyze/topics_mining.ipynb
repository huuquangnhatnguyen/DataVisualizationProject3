{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess dialogue text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove scene directions in parentheses or brackets\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaker(person_scene):\n",
    "    \"\"\"Extract the speaker name from the person_scene column\"\"\"\n",
    "    if not isinstance(person_scene, str):\n",
    "        return None\n",
    "    \n",
    "    # Extract the speaker name (assuming format is \"Name\" or \"Name:\")\n",
    "    match = re.match(r'^([^:]+)', person_scene.strip())\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sheldon_conversations(df, main_cast):\n",
    "    \"\"\"\n",
    "    Extract conversations between Sheldon and main cast members\n",
    "    Returns a dictionary with cast member names as keys and lists of conversations as values\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store conversations\n",
    "    sheldon_conversations = {member: [] for member in main_cast if member != \"Sheldon\"}\n",
    "    sheldon_conversations[\"all\"] = []  # For all Sheldon dialogues\n",
    "    \n",
    "    # Sort by series, episode, and implied sequence\n",
    "    df = df.sort_values(by=[\"series\", \"episode\"])\n",
    "    \n",
    "    # Group by episode\n",
    "    episode_groups = df.groupby([\"series\", \"episode\"])\n",
    "    \n",
    "    for (series, episode), episode_df in episode_groups:\n",
    "        # Reset index to get sequence within episode\n",
    "        episode_df = episode_df.reset_index(drop=True)\n",
    "        \n",
    "        # Process each line in the episode\n",
    "        for i in range(len(episode_df) - 1):  # -1 to avoid index out of bounds\n",
    "            current_row = episode_df.iloc[i]\n",
    "            next_row = episode_df.iloc[i + 1]\n",
    "            \n",
    "            # Extract speaker names\n",
    "            current_speaker = extract_speaker(current_row[\"person_scene\"])\n",
    "            next_speaker = extract_speaker(next_row[\"person_scene\"])\n",
    "            \n",
    "            # Skip if either speaker is not identifiable\n",
    "            if not current_speaker or not next_speaker:\n",
    "                continue\n",
    "            \n",
    "            # If Sheldon is speaking\n",
    "            if current_speaker == \"Sheldon\":\n",
    "                # Add to all Sheldon dialogues\n",
    "                cleaned_dialogue = preprocess_text(current_row[\"dialogue\"])\n",
    "                if cleaned_dialogue:\n",
    "                    sheldon_conversations[\"all\"].append(cleaned_dialogue)\n",
    "                \n",
    "                # If the next speaker is a main cast member\n",
    "                if next_speaker in main_cast and next_speaker != \"Sheldon\":\n",
    "                    sheldon_line = preprocess_text(current_row[\"dialogue\"])\n",
    "                    response_line = preprocess_text(next_row[\"dialogue\"])\n",
    "                    \n",
    "                    # Add the dialogue pair to the corresponding cast member's list\n",
    "                    if sheldon_line and response_line:\n",
    "                        sheldon_conversations[next_speaker].append(\n",
    "                            {\"sheldon\": sheldon_line, \"response\": response_line}\n",
    "                        )\n",
    "            \n",
    "            # If a main cast member is speaking and Sheldon responds\n",
    "            elif current_speaker in main_cast and next_speaker == \"Sheldon\":\n",
    "                cast_line = preprocess_text(current_row[\"dialogue\"])\n",
    "                sheldon_line = preprocess_text(next_row[\"dialogue\"])\n",
    "                \n",
    "                # Add to all Sheldon dialogues\n",
    "                if sheldon_line:\n",
    "                    sheldon_conversations[\"all\"].append(sheldon_line)\n",
    "                \n",
    "                # Add the dialogue pair to the corresponding cast member's list\n",
    "                if cast_line and sheldon_line:\n",
    "                    sheldon_conversations[current_speaker].append(\n",
    "                        {\"cast\": cast_line, \"sheldon\": sheldon_line}\n",
    "                    )\n",
    "    \n",
    "    return sheldon_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bertopic_analysis(conversations, output_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Run BERTopic analysis on the conversations\n",
    "    \n",
    "    Args:\n",
    "        conversations: List of text documents to analyze\n",
    "        output_prefix: Prefix for output files\n",
    "    \n",
    "    Returns:\n",
    "        BERTopic model and topics\n",
    "    \"\"\"\n",
    "    # Initialize sentence transformer model\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Initialize dimensionality reduction with lower n_components to avoid the eigenvalue error\n",
    "    # Reducing n_components to 2 instead of 5\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    \n",
    "    # Initialize clustering model with a smaller min_cluster_size\n",
    "    # Change min_cluster_size from 15 to a smaller value if dataset is small\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    \n",
    "    # Initialize vectorizer with n-grams\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2), min_df=0.01, max_df=0.9)\n",
    "    \n",
    "    # Initialize BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit model and transform documents\n",
    "    topics, probs = topic_model.fit_transform(conversations)\n",
    "    \n",
    "    n_real_topics = len({t for t in topics if t != -1})   # exclude outliers (-1)\n",
    "    \n",
    "    # Modify visualization approach to avoid the error\n",
    "    if n_real_topics > 2:\n",
    "        try:\n",
    "            # Try to visualize topics, but with a contingency plan\n",
    "            fig = topic_model.visualize_topics()\n",
    "            fig.write_html(f\"{output_prefix}topic_visualization.html\")\n",
    "        except TypeError as e:\n",
    "            # If we get the eigenvalue error, handle it gracefully\n",
    "            print(f\"Visualization error: {e}\")\n",
    "            print(\"Skipping interactive visualization, but still saving topic information.\")\n",
    "    else:\n",
    "        print(f\"{n_real_topics} topic(s) detected – skipping 2-D visualization.\")\n",
    "\n",
    "    # 4. exports --------------------------------------------------------------\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topic_info.to_csv(f\"{output_prefix}topic_info.csv\", index=False)\n",
    "\n",
    "    with open(f\"{output_prefix}top_topics.txt\", \"w\") as f:\n",
    "        for topic in topic_info.itertuples():\n",
    "            if topic.Topic != -1:\n",
    "                f.write(f\"Topic {topic.Topic}: {topic.Name}\\n\")\n",
    "                words = topic_model.get_topic(topic.Topic)\n",
    "                f.write(\"Key terms: \" +\n",
    "                        \", \".join([f\"{w} ({s:.3f})\" for w, s in words[:10]]) +\n",
    "                        \"\\n\\n\")\n",
    "\n",
    "    return topic_model, topics\n",
    "\n",
    "# Alternative approach if you need to modify analyze_sheldon_topics function\n",
    "def analyze_sheldon_topics(csv_file):\n",
    "    \"\"\"Main function to analyze topics in Sheldon's conversations\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Define main cast\n",
    "    main_cast = [\"Sheldon\", \"Leonard\", \"Penny\", \"Howard\", \"Raj\", \"Amy\", \"Bernadette\"]\n",
    "    \n",
    "    print(\"Extracting conversations...\")\n",
    "    conversations = extract_sheldon_conversations(df, main_cast)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nConversation Statistics:\")\n",
    "    print(f\"Total Sheldon dialogues: {len(conversations['all'])}\")\n",
    "    for member in main_cast:\n",
    "        if member != \"Sheldon\":\n",
    "            print(f\"Sheldon-{member} conversations: {len(conversations[member])}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    import os\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Analyze all Sheldon dialogues\n",
    "    print(\"\\nAnalyzing all Sheldon dialogues...\")\n",
    "    sheldon_model, sheldon_topics = run_bertopic_analysis(\n",
    "        conversations[\"all\"], \n",
    "        output_prefix=\"results/all_sheldon_\"\n",
    "    )\n",
    "    \n",
    "    # For very small datasets, we might need to skip topic modeling\n",
    "    # Adding a minimum threshold (e.g., 50 conversations)\n",
    "    min_samples_for_topics = 100\n",
    "    \n",
    "    # Analyze Sheldon's conversations with each main cast member\n",
    "    for member in main_cast:\n",
    "        if member != \"Sheldon\":\n",
    "            # Skip if not enough conversations\n",
    "            if len(conversations[member]) < min_samples_for_topics:\n",
    "                print(f\"Skipping {member} due to insufficient data (less than {min_samples_for_topics} samples)\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nAnalyzing Sheldon-{member} conversations...\")\n",
    "            # Extract Sheldon's lines from conversations with this cast member\n",
    "            sheldon_lines = []\n",
    "            for conv in conversations[member]:\n",
    "                if \"sheldon\" in conv:\n",
    "                    sheldon_lines.append(conv[\"sheldon\"])\n",
    "            \n",
    "            if len(sheldon_lines) < min_samples_for_topics:\n",
    "                print(f\"Skipping {member} due to insufficient data after preprocessing\")\n",
    "                continue\n",
    "                \n",
    "            # Run topic modeling with try/except to handle potential errors\n",
    "            try:\n",
    "                member_model, member_topics = run_bertopic_analysis(\n",
    "                    sheldon_lines,\n",
    "                    output_prefix=f\"results/sheldon_{member.lower()}_\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {member} conversations: {str(e)}\")\n",
    "                print(f\"Skipping {member} analysis.\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete. Results saved to 'results' directory.\")\n",
    "    \n",
    "    # Return the main model for further analysis if needed\n",
    "    return sheldon_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_topic_evolution(csv_file, topic_model=None):\n",
    "    \"\"\"\n",
    "    Analyze how topics evolve across different seasons.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to the CSV file containing script data\n",
    "        topic_model: An existing BERTopic model (if None, a new model will be created)\n",
    "        \n",
    "    Returns:\n",
    "        Visualizations and data showing topic trends over seasons\n",
    "    \"\"\"\n",
    "    print(\"Analyzing topic evolution across seasons...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Define main cast\n",
    "    main_cast = [\"Sheldon\", \"Leonard\", \"Penny\", \"Howard\", \"Raj\", \"Amy\", \"Bernadette\"]\n",
    "    \n",
    "    # Filter for Sheldon's dialogues only\n",
    "    sheldon_df = df[df[\"person_scene\"].apply(\n",
    "        lambda x: isinstance(x, str) and extract_speaker(x) == \"Sheldon\"\n",
    "    )].copy()\n",
    "    \n",
    "    # Clean dialogues\n",
    "    sheldon_df[\"clean_dialogue\"] = sheldon_df[\"dialogue\"].apply(\n",
    "        lambda x: preprocess_text(x) if isinstance(x, str) else \"\"\n",
    "    )\n",
    "    \n",
    "    # Remove empty dialogues\n",
    "    sheldon_df = sheldon_df[sheldon_df[\"clean_dialogue\"] != \"\"]\n",
    "    \n",
    "    # Group by season\n",
    "    season_groups = sheldon_df.groupby(\"series\")\n",
    "    \n",
    "    # Track topics by season\n",
    "    seasons = sorted(sheldon_df[\"series\"].unique())\n",
    "    season_dialogues = {}\n",
    "    \n",
    "    for season in seasons:\n",
    "        if season in season_groups.groups:\n",
    "            season_data = season_groups.get_group(season)\n",
    "            season_dialogues[season] = season_data[\"clean_dialogue\"].tolist()\n",
    "    \n",
    "    # Create a new topic model if one isn't provided\n",
    "    if topic_model is None:\n",
    "        print(\"Creating new topic model for all seasons combined...\")\n",
    "        all_dialogues = [d for s in season_dialogues.values() for d in s]\n",
    "        topic_model, _ = run_bertopic_analysis(all_dialogues, \"results/all_seasons_\")\n",
    "    \n",
    "    # Create output directory\n",
    "    import os\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Analyze each season with the same model\n",
    "    print(\"Analyzing topics by season...\")\n",
    "    season_topics = {}\n",
    "    season_topic_counts = {}\n",
    "    \n",
    "    for season, dialogues in season_dialogues.items():\n",
    "        if len(dialogues) < 30:  # Skip seasons with too few dialogues\n",
    "            print(f\"Skipping season {season} (insufficient data)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing season {season} ({len(dialogues)} dialogues)\")\n",
    "        \n",
    "        # Use the same model to transform this season's dialogues\n",
    "        topics, probs = topic_model.transform(dialogues)\n",
    "        \n",
    "        # Store the topics\n",
    "        season_topics[season] = topics\n",
    "        \n",
    "        # Count topic occurrences\n",
    "        topic_counts = pd.Series(topics).value_counts()\n",
    "        season_topic_counts[season] = topic_counts\n",
    "    \n",
    "    # Get topic info\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Create a mapping of topic IDs to names\n",
    "    topic_names = {}\n",
    "    for _, row in topic_info.iterrows():\n",
    "        if row[\"Topic\"] != -1:  # Skip outlier topic\n",
    "            # Get the topic words\n",
    "            words = [word for word, _ in topic_model.get_topic(row[\"Topic\"])[:3]]\n",
    "            topic_names[row[\"Topic\"]] = f\"Topic {row['Topic']}: {' '.join(words)}\"\n",
    "        else:\n",
    "            topic_names[-1] = \"Outliers\"\n",
    "    \n",
    "    # Create a dataframe for the evolution of topics\n",
    "    evolution_data = []\n",
    "    \n",
    "    for season, counts in season_topic_counts.items():\n",
    "        for topic, count in counts.items():\n",
    "            # Skip outlier topic for cleaner visualization\n",
    "            if topic != -1:\n",
    "                name = topic_names.get(topic, f\"Topic {topic}\")\n",
    "                proportion = count / len(season_dialogues[season])\n",
    "                evolution_data.append({\n",
    "                    \"Season\": season,\n",
    "                    \"Topic\": topic,\n",
    "                    \"Topic Name\": name,\n",
    "                    \"Count\": count,\n",
    "                    \"Proportion\": proportion\n",
    "                })\n",
    "    \n",
    "    evolution_df = pd.DataFrame(evolution_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    evolution_df.to_csv(\"results/topic_evolution.csv\", index=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    \n",
    "    # 1. Line plot showing topic trends over seasons\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Get top 10 topics by overall count\n",
    "    top_topics = evolution_df.groupby(\"Topic\")[\"Count\"].sum().nlargest(10).index\n",
    "    \n",
    "    # Filter for top topics and create pivot table\n",
    "    top_evolution_df = evolution_df[evolution_df[\"Topic\"].isin(top_topics)]\n",
    "    pivot_df = top_evolution_df.pivot_table(\n",
    "        index=\"Season\", \n",
    "        columns=\"Topic Name\", \n",
    "        values=\"Proportion\",\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Plot trends\n",
    "    ax = pivot_df.plot(kind=\"line\", marker=\"o\", ax=plt.gca())\n",
    "    plt.title(\"Evolution of Top Topics Across Seasons\", fontsize=16)\n",
    "    plt.xlabel(\"Season\", fontsize=14)\n",
    "    plt.ylabel(\"Topic Proportion\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title=\"Topics\", fontsize=10)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(\"results/topic_evolution_line.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap of all topics across seasons\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Create pivot table with all non-outlier topics\n",
    "    non_outlier_df = evolution_df[evolution_df[\"Topic\"] != -1]\n",
    "    pivot_all = non_outlier_df.pivot_table(\n",
    "        index=\"Topic Name\", \n",
    "        columns=\"Season\", \n",
    "        values=\"Proportion\",\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Sort rows by sum of proportions\n",
    "    pivot_all = pivot_all.loc[pivot_all.sum(axis=1).sort_values(ascending=False).index]\n",
    "    \n",
    "    # Keep only top 20 topics for readability\n",
    "    if len(pivot_all) > 20:\n",
    "        pivot_all = pivot_all.iloc[:20]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_all, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=0.5)\n",
    "    plt.title(\"Topic Proportion Heatmap by Season\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save heatmap\n",
    "    plt.savefig(\"results/topic_heatmap_by_season.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Character-specific topic evolution (for top characters)\n",
    "    top_chars = [\"Leonard\", \"Penny\", \"Amy\"]\n",
    "    \n",
    "    for char in top_chars:\n",
    "        print(f\"Analyzing Sheldon-{char} conversations across seasons...\")\n",
    "        \n",
    "        # Filter conversations where Sheldon interacts with this character\n",
    "        char_df = df.copy()\n",
    "        \n",
    "        # Group by episode to find interactions\n",
    "        episode_groups = char_df.groupby([\"series\", \"episode\"])\n",
    "        \n",
    "        # Store dialogues by season\n",
    "        char_season_dialogues = {season: [] for season in seasons}\n",
    "        \n",
    "        for (season, episode), episode_df in episode_groups:\n",
    "            # Reset index for proper sequence\n",
    "            episode_df = episode_df.reset_index(drop=True)\n",
    "            \n",
    "            # Find Sheldon's lines when talking with this character\n",
    "            for i in range(len(episode_df) - 1):\n",
    "                current_row = episode_df.iloc[i]\n",
    "                next_row = episode_df.iloc[i + 1]\n",
    "                \n",
    "                # Extract speakers\n",
    "                current_speaker = extract_speaker(current_row.get(\"person_scene\", \"\"))\n",
    "                next_speaker = extract_speaker(next_row.get(\"person_scene\", \"\"))\n",
    "                \n",
    "                if current_speaker == \"Sheldon\" and next_speaker == char:\n",
    "                    dialogue = preprocess_text(current_row.get(\"dialogue\", \"\"))\n",
    "                    if dialogue:\n",
    "                        if season in char_season_dialogues:\n",
    "                            char_season_dialogues[season].append(dialogue)\n",
    "                \n",
    "                elif current_speaker == char and next_speaker == \"Sheldon\":\n",
    "                    dialogue = preprocess_text(next_row.get(\"dialogue\", \"\"))\n",
    "                    if dialogue:\n",
    "                        if season in char_season_dialogues:\n",
    "                            char_season_dialogues[season].append(dialogue)\n",
    "        \n",
    "        # Analyze topics for each season\n",
    "        char_evolution_data = []\n",
    "        \n",
    "        for season, dialogues in char_season_dialogues.items():\n",
    "            if len(dialogues) < 30:  # Skip if not enough data\n",
    "                continue\n",
    "                \n",
    "            # Transform dialogues using existing model\n",
    "            topics, _ = topic_model.transform(dialogues)\n",
    "            \n",
    "            # Count topics\n",
    "            topic_counts = pd.Series(topics).value_counts()\n",
    "            \n",
    "            # Store data\n",
    "            for topic, count in topic_counts.items():\n",
    "                if topic != -1:  # Skip outliers\n",
    "                    name = topic_names.get(topic, f\"Topic {topic}\")\n",
    "                    proportion = count / len(dialogues)\n",
    "                    char_evolution_data.append({\n",
    "                        \"Season\": season,\n",
    "                        \"Topic\": topic,\n",
    "                        \"Topic Name\": name,\n",
    "                        \"Count\": count,\n",
    "                        \"Proportion\": proportion,\n",
    "                        \"Character\": char\n",
    "                    })\n",
    "        \n",
    "        # Create dataframe\n",
    "        if char_evolution_data:\n",
    "            char_evolution_df = pd.DataFrame(char_evolution_data)\n",
    "            \n",
    "            # Create heatmap\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            \n",
    "            char_pivot = char_evolution_df.pivot_table(\n",
    "                index=\"Topic Name\", \n",
    "                columns=\"Season\", \n",
    "                values=\"Proportion\",\n",
    "                fill_value=0\n",
    "            )\n",
    "            \n",
    "            # Sort rows by sum of proportions\n",
    "            char_pivot = char_pivot.loc[char_pivot.sum(axis=1).sort_values(ascending=False).index]\n",
    "            \n",
    "            # Keep only top 15 topics for readability\n",
    "            if len(char_pivot) > 15:\n",
    "                char_pivot = char_pivot.iloc[:15]\n",
    "            \n",
    "            # Create heatmap\n",
    "            sns.heatmap(char_pivot, annot=True, fmt=\".2f\", cmap=\"YlOrRd\", linewidths=0.5)\n",
    "            plt.title(f\"Sheldon-{char} Topic Evolution by Season\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save heatmap\n",
    "            plt.savefig(f\"results/sheldon_{char.lower()}_topic_evolution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    \n",
    "    # Return the combined results dataframe for further analysis\n",
    "    return evolution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topics_csv(csv_file, topic_model):\n",
    "    \"\"\"\n",
    "    Generate a CSV file showing topics over seasons between Sheldon and other characters\n",
    "    in the format: year,character,topic_name,n,prop\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to the CSV file containing script data\n",
    "        topic_model: An existing BERTopic model\n",
    "    \"\"\"\n",
    "    print(\"Generating topics CSV...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Define main cast\n",
    "    main_cast = [\"Leonard\", \"Penny\", \"Howard\", \"Raj\", \"Amy\", \"Bernadette\"]\n",
    "    \n",
    "    # Create output directory\n",
    "    import os\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Initialize a list to store all data\n",
    "    all_topics_data = []\n",
    "    \n",
    "    # Get topic info\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Create a mapping of topic IDs to names\n",
    "    topic_names = {}\n",
    "    for _, row in topic_info.iterrows():\n",
    "        if row[\"Topic\"] != -1:  # Skip outlier topic\n",
    "            # Get the topic words\n",
    "            words = [word for word, _ in topic_model.get_topic(row[\"Topic\"])[:3]]\n",
    "            topic_names[row[\"Topic\"]] = \" \".join(words)\n",
    "        else:\n",
    "            topic_names[-1] = \"outliers\"\n",
    "    \n",
    "    # Process each character's conversations with Sheldon by season\n",
    "    for char in main_cast:\n",
    "        print(f\"Processing Sheldon-{char} conversations...\")\n",
    "        \n",
    "        # Filter conversations where Sheldon interacts with this character\n",
    "        char_df = df.copy()\n",
    "        \n",
    "        # Group by episode to find interactions\n",
    "        episode_groups = char_df.groupby([\"series\", \"episode\"])\n",
    "        \n",
    "        # Store dialogues by season\n",
    "        seasons = sorted(char_df[\"series\"].unique())\n",
    "        char_season_dialogues = {season: [] for season in seasons}\n",
    "        \n",
    "        for (season, episode), episode_df in episode_groups:\n",
    "            # Reset index for proper sequence\n",
    "            episode_df = episode_df.reset_index(drop=True)\n",
    "            \n",
    "            # Find Sheldon's lines when talking with this character\n",
    "            for i in range(len(episode_df) - 1):\n",
    "                current_row = episode_df.iloc[i]\n",
    "                next_row = episode_df.iloc[i + 1]\n",
    "                \n",
    "                # Extract speakers\n",
    "                current_speaker = extract_speaker(current_row.get(\"person_scene\", \"\"))\n",
    "                next_speaker = extract_speaker(next_row.get(\"person_scene\", \"\"))\n",
    "                \n",
    "                if current_speaker == \"Sheldon\" and next_speaker == char:\n",
    "                    dialogue = preprocess_text(current_row.get(\"dialogue\", \"\"))\n",
    "                    if dialogue:\n",
    "                        if season in char_season_dialogues:\n",
    "                            char_season_dialogues[season].append(dialogue)\n",
    "                \n",
    "                elif current_speaker == char and next_speaker == \"Sheldon\":\n",
    "                    dialogue = preprocess_text(next_row.get(\"dialogue\", \"\"))\n",
    "                    if dialogue:\n",
    "                        if season in char_season_dialogues:\n",
    "                            char_season_dialogues[season].append(dialogue)\n",
    "        \n",
    "        # Analyze topics for each season\n",
    "        for season, dialogues in char_season_dialogues.items():\n",
    "            if len(dialogues) < 30:  # Skip if not enough data\n",
    "                continue\n",
    "                \n",
    "            # Transform dialogues using existing model\n",
    "            topics, _ = topic_model.transform(dialogues)\n",
    "            \n",
    "            # Count topics\n",
    "            topic_counts = pd.Series(topics).value_counts()\n",
    "            \n",
    "            # Store data\n",
    "            for topic, count in topic_counts.items():\n",
    "                if topic != -1:  # Skip outliers\n",
    "                    name = topic_names.get(topic, f\"Topic_{topic}\")\n",
    "                    proportion = count / len(dialogues)\n",
    "                    all_topics_data.append({\n",
    "                        \"year\": int(season) if season.is_integer() else season,\n",
    "                        \"character\": char,\n",
    "                        \"name\": name,\n",
    "                        \"n\": count,\n",
    "                        \"prop\": proportion\n",
    "                    })\n",
    "    \n",
    "    # Create dataframe\n",
    "    topics_df = pd.DataFrame(all_topics_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    topics_df.to_csv(\"results/sheldon_character_topics.csv\", index=False)\n",
    "    \n",
    "    print(f\"CSV file created: results/sheldon_character_topics.csv\")\n",
    "    \n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Extracting conversations...\n",
      "\n",
      "Conversation Statistics:\n",
      "Total Sheldon dialogues: 20659\n",
      "Sheldon-Leonard conversations: 6758\n",
      "Sheldon-Penny conversations: 4417\n",
      "Sheldon-Howard conversations: 1863\n",
      "Sheldon-Raj conversations: 1668\n",
      "Sheldon-Amy conversations: 3221\n",
      "Sheldon-Bernadette conversations: 289\n",
      "\n",
      "Analyzing all Sheldon dialogues...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 01:18:57,317 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 646/646 [00:07<00:00, 87.28it/s] \n",
      "2025-04-30 01:19:04,876 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-30 01:19:04,877 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-30 01:19:49,303 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:19:49,306 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-30 01:19:52,593 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-30 01:19:52,604 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-30 01:19:53,316 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Sheldon-Leonard conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 01:20:06,367 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 212/212 [00:02<00:00, 87.32it/s] \n",
      "2025-04-30 01:20:08,851 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-30 01:20:08,852 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-30 01:20:16,153 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:20:16,155 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-30 01:20:16,299 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-30 01:20:16,302 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-30 01:20:16,506 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Sheldon-Penny conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 01:20:17,999 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 139/139 [00:01<00:00, 88.67it/s] \n",
      "2025-04-30 01:20:19,604 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-30 01:20:19,605 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-30 01:20:24,383 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:20:24,384 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-30 01:20:24,477 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-30 01:20:24,480 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-30 01:20:24,623 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Sheldon-Howard conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 01:20:25,856 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 59/59 [00:00<00:00, 86.57it/s]\n",
      "2025-04-30 01:20:26,557 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-30 01:20:26,557 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-30 01:20:30,511 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:20:30,511 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-30 01:20:30,551 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-30 01:20:30,553 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-30 01:20:30,632 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Sheldon-Raj conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 01:20:31,703 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 53/53 [00:00<00:00, 82.21it/s]\n",
      "2025-04-30 01:20:32,366 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-30 01:20:32,367 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-30 01:20:35,640 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:20:35,641 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-30 01:20:35,676 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-30 01:20:35,679 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-30 01:20:35,772 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Sheldon-Amy conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 01:20:36,921 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 101/101 [00:01<00:00, 88.23it/s]\n",
      "2025-04-30 01:20:38,095 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-30 01:20:38,096 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-30 01:20:47,668 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:20:47,669 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-30 01:20:47,741 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-30 01:20:47,744 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-30 01:20:47,849 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Sheldon-Bernadette conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 01:20:49,092 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 10/10 [00:00<00:00, 72.38it/s]\n",
      "2025-04-30 01:20:49,236 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-30 01:20:49,238 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-30 01:20:49,552 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:20:49,553 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-30 01:20:49,561 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-30 01:20:49,564 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-30 01:20:49,577 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 topic(s) detected – skipping 2-D visualization.\n",
      "\n",
      "Analysis complete. Results saved to 'results' directory.\n",
      "Analyzing topic evolution across seasons...\n",
      "Analyzing topics by season...\n",
      "Processing season 1.0 (1096 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 35/35 [00:00<00:00, 86.43it/s]\n",
      "2025-04-30 01:20:50,226 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:08,035 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:08,036 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:08,081 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 2.0 (1343 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 42/42 [00:00<00:00, 77.93it/s]\n",
      "2025-04-30 01:21:08,642 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:09,080 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:09,081 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:09,130 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 3.0 (1307 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 41/41 [00:00<00:00, 80.39it/s]\n",
      "2025-04-30 01:21:09,662 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:10,082 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:10,083 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:10,133 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 4.0 (1328 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 42/42 [00:00<00:00, 87.84it/s]\n",
      "2025-04-30 01:21:10,632 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:11,083 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:11,084 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:11,134 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 5.0 (1019 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:00<00:00, 79.61it/s]\n",
      "2025-04-30 01:21:11,556 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:11,912 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:11,913 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:11,950 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 6.0 (987 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31/31 [00:00<00:00, 85.46it/s]\n",
      "2025-04-30 01:21:12,331 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:12,695 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:12,696 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:12,735 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 7.0 (1092 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 35/35 [00:00<00:00, 81.23it/s]\n",
      "2025-04-30 01:21:13,186 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:13,573 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:13,574 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:13,619 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 8.0 (1096 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 35/35 [00:00<00:00, 84.94it/s]\n",
      "2025-04-30 01:21:14,050 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:14,446 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:14,447 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:14,491 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 9.0 (1098 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 35/35 [00:00<00:00, 83.93it/s]\n",
      "2025-04-30 01:21:14,926 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:15,329 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:15,330 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:15,372 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season 10.0 (1071 dialogues)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 34/34 [00:00<00:00, 90.58it/s]\n",
      "2025-04-30 01:21:15,765 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:16,168 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:16,169 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:16,211 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Sheldon-Leonard conversations across seasons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:00<00:00, 78.27it/s]\n",
      "2025-04-30 01:21:22,296 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:22,613 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:22,614 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:22,650 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 28/28 [00:00<00:00, 89.20it/s]\n",
      "2025-04-30 01:21:22,982 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:23,270 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:23,270 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:23,302 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 80.10it/s]\n",
      "2025-04-30 01:21:23,631 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:23,893 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:23,894 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:23,923 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 27/27 [00:00<00:00, 91.69it/s]\n",
      "2025-04-30 01:21:24,235 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:24,537 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:24,537 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:24,568 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 21/21 [00:00<00:00, 76.22it/s]\n",
      "2025-04-30 01:21:24,859 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:25,101 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:25,102 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:25,126 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 78.15it/s]\n",
      "2025-04-30 01:21:25,330 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:25,508 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:25,509 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:25,527 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 17/17 [00:00<00:00, 84.01it/s]\n",
      "2025-04-30 01:21:25,742 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:25,932 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:25,933 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:25,954 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 21/21 [00:00<00:00, 91.61it/s]\n",
      "2025-04-30 01:21:26,195 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:26,433 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:26,434 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:26,458 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 17/17 [00:00<00:00, 87.70it/s]\n",
      "2025-04-30 01:21:26,667 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:26,872 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:26,873 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:26,892 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 13/13 [00:00<00:00, 93.66it/s]\n",
      "2025-04-30 01:21:27,043 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:27,203 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:27,204 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:27,219 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Sheldon-Penny conversations across seasons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 83.40it/s]\n",
      "2025-04-30 01:21:31,733 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:31,899 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:31,900 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:31,920 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 26/26 [00:00<00:00, 85.21it/s]\n",
      "2025-04-30 01:21:32,238 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:32,523 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:32,524 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:32,554 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 22/22 [00:00<00:00, 91.10it/s]\n",
      "2025-04-30 01:21:32,808 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:33,043 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:33,044 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:33,070 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 17/17 [00:00<00:00, 94.78it/s]\n",
      "2025-04-30 01:21:33,261 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:33,460 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:33,461 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:33,481 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 80.33it/s]\n",
      "2025-04-30 01:21:33,591 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:33,701 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:33,702 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:33,715 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 59.68it/s]\n",
      "2025-04-30 01:21:33,876 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:34,026 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:34,027 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:34,040 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 14/14 [00:00<00:00, 79.12it/s]\n",
      "2025-04-30 01:21:34,227 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:34,445 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:34,446 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:34,468 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 75.20it/s]\n",
      "2025-04-30 01:21:34,640 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:34,819 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:34,820 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:34,836 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 81.97it/s]\n",
      "2025-04-30 01:21:34,956 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:35,092 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:35,093 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:35,104 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 85.42it/s]\n",
      "2025-04-30 01:21:35,206 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:35,345 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:35,346 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:35,359 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Sheldon-Amy conversations across seasons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 11/11 [00:00<00:00, 78.25it/s]\n",
      "2025-04-30 01:21:39,793 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:39,937 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:39,938 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:39,952 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 77.76it/s]\n",
      "2025-04-30 01:21:40,077 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:40,196 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:40,197 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:40,208 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 13/13 [00:00<00:00, 77.35it/s]\n",
      "2025-04-30 01:21:40,386 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:40,537 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:40,538 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:40,553 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 14/14 [00:00<00:00, 84.54it/s]\n",
      "2025-04-30 01:21:40,730 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:40,899 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:40,900 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:40,916 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 86.18it/s]\n",
      "2025-04-30 01:21:41,101 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:41,272 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:41,273 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:41,290 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 81.11it/s]\n",
      "2025-04-30 01:21:41,498 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:41,688 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:41,689 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:41,709 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 27/27 [00:00<00:00, 88.49it/s]\n",
      "2025-04-30 01:21:42,029 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-30 01:21:42,351 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-30 01:21:42,353 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-30 01:21:42,384 - BERTopic - Cluster - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Replace with your file path\n",
    "\n",
    "\n",
    "    file = \"big_bang_scripts.csv\"\n",
    "    \n",
    "    # First run the regular analysis\n",
    "    topic_model = analyze_sheldon_topics(file)\n",
    "    \n",
    "    # Then analyze evolution over time using the same model\n",
    "    evolution_df = analyze_topic_evolution(file, topic_model)\n",
    "\n",
    "    # Generate the character-topic CSV in the requested format\n",
    "    topics_df = generate_topics_csv(file, topic_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
